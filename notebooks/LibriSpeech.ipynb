{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5hvo8QWN-a9"
   },
   "source": [
    "# Installing Whisper\n",
    "\n",
    "The commands below will install the Python packages needed to use Whisper models and evaluate the transcription results."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "ZsJUxc0aRsAf"
   },
   "source": [
    "! pip install git+https://github.com/openai/whisper.git\n",
    "! pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IMEkgyagYto"
   },
   "source": [
    "# Loading the LibriSpeech dataset\n",
    "\n",
    "The following will load the test-clean split of the LibriSpeech corpus using torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\\\work\\\\Github\\\\whisper')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CqtR2Fi5-vP"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuCCB2KYOJCE"
   },
   "outputs": [],
   "source": [
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap LibriSpeech and trim/pad the audio to 30 seconds.\n",
    "    It will drop the last few seconds of a very small portion of the utterances.\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"test-clean\", device=DEVICE):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(\n",
    "            root=os.path.expanduser(\"~/.cache\"),\n",
    "            url=split,\n",
    "            download=True,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        audio, sample_rate, text, _, _, _ = self.dataset[item]\n",
    "        assert sample_rate == 16000\n",
    "        audio = whisper.pad_or_trim(audio.flatten()).to(self.device)\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        return (mel, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YcRU5jqNqo2"
   },
   "outputs": [],
   "source": [
    "dataset = LibriSpeech(\"test-clean\")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ljocCNuUAde"
   },
   "source": [
    "# Running inference on the dataset using a base Whisper model\n",
    "\n",
    "The following will take a few minutes to transcribe all utterances in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_PokfNJtOYNu",
    "outputId": "2c53ec44-bc93-4107-b4fa-214e3f71fe8e"
   },
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base.en\")\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict without timestamps for short-form transcription\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True, fp16 = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "09a29a91f58d4462942505a3cc415801",
      "83391f98a240490987c397048fc1a0d4",
      "06b9aa5f49fa44ba8c93b647dc7db224",
      "da9c231ee67047fb89073c95326b72a5",
      "48da931ebe7f4fd299f8c98c7d2460ff",
      "7a901f447c1d477bb49f954e0feacedd",
      "39f5a6ae8ba74c8598f9c6d5b8ad2d65",
      "a0d10a42c753453283e5219c22239337",
      "09f4cb79ff86465aaf48b0de24869af9",
      "1b9cecf5b3584fba8258a81d4279a25b",
      "039b53f2702c4179af7e0548018d0588"
     ]
    },
    "id": "7OWTn_KvNk59",
    "outputId": "a813a792-3c91-4144-f11f-054fd6778023"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hypotheses = []\n",
    "references = []\n",
    "\n",
    "for mels, texts in tqdm(loader):\n",
    "    results = model.decode(mels, options)\n",
    "    hypotheses.extend([result.text for result in results])\n",
    "    references.extend(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4nTyynELQ42j",
    "outputId": "1c72d25a-3e87-4c60-a8d1-1da9d2f73bd7"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(dict(hypothesis=hypotheses, reference=references))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPppEJRXX4ox"
   },
   "source": [
    "# Calculating the word error rate\n",
    "\n",
    "Now, we use our English normalizer implementation to standardize the transcription and calculate the WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl-KBDflMhrg"
   },
   "outputs": [],
   "source": [
    "import jiwer\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "\n",
    "normalizer = EnglishTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "6-O048q4WI4o",
    "outputId": "f2089bc9-f535-441e-f192-26e52ae82b5e"
   },
   "outputs": [],
   "source": [
    "data[\"hypothesis_clean\"] = [normalizer(text) for text in data[\"hypothesis\"]]\n",
    "data[\"reference_clean\"] = [normalizer(text) for text in data[\"reference\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBGSITeBYPTT",
    "outputId": "7b3dbe7c-a37e-4a07-a50a-b27d5f88b68f"
   },
   "outputs": [],
   "source": [
    "wer = jiwer.wer(list(data[\"reference_clean\"]), list(data[\"hypothesis_clean\"]))\n",
    "\n",
    "print(f\"WER: {wer * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "for mels, texts, audio in loader:\n",
    "    print(type(mels), mels.shape)\n",
    "    print(type(texts))\n",
    "    print(type(audio), audio.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sample_rate, text, _, _, _  = dataset.dataset[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hypotheses = []\n",
    "print(type(audio), audio.shape, f\"{audio.shape[1]/16000}\")\n",
    "w_audio = whisper.pad_or_trim(audio.flatten())\n",
    "print(type(w_audio), w_audio.shape)\n",
    "mel = whisper.log_mel_spectrogram(w_audio)\n",
    "print(type(mel), mel.shape)\n",
    "results = model.decode(mel, options)\n",
    "print(f\"|{results.text}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  try different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tiny = whisper.load_model(\"tiny\")\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model_tiny.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model_tiny.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "audio, sample_rate, text, _, _, _  = dataset.dataset[0]\n",
    "\n",
    "hypotheses = []\n",
    "print(type(audio), audio.shape, f\"{audio.shape[1]/16000}\")\n",
    "w_audio = whisper.pad_or_trim(audio.flatten())\n",
    "print(type(w_audio), w_audio.shape)\n",
    "mel = whisper.log_mel_spectrogram(w_audio)\n",
    "print(type(mel), mel.shape)\n",
    "results = model_tiny.decode(mel, options)\n",
    "#hypotheses.extend([result.text for result in results])\n",
    "print(f\"|{results.text}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and transcribe local file (16Khz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def load_wav_to_tensor(file_path):\n",
    "    # Load the WAV file using librosa\n",
    "    waveform, sample_rate = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "    # Convert the waveform to a torch tensor\n",
    "    tensor_waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "    return tensor_waveform, sample_rate\n",
    "\n",
    "# Example usage\n",
    "file_path = r\"C:\\work\\local4test\\sampleAudio\\shortTestRecording.wav\"\n",
    "audio, sample_rate = load_wav_to_tensor(file_path)\n",
    "\n",
    "print(type(audio), audio.shape, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hypotheses = []\n",
    "print(type(audio), audio.shape, f\"{audio.shape[1]/16000}\")\n",
    "w_audio = whisper.pad_or_trim(audio.flatten())\n",
    "print(type(w_audio), w_audio.shape)\n",
    "mel = whisper.log_mel_spectrogram(w_audio)\n",
    "print(type(mel), mel.shape)\n",
    "results = model_tiny.decode(mel, options)\n",
    "#hypotheses.extend([result.text for result in results])\n",
    "print(f\"|{results.text}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [decode code ](https://github.com/openai/whisper/blob/main/whisper/decoding.py)\n",
    "\n",
    "\n",
    "# fast decode\n",
    "\n",
    "https://github.com/openai/whisper/discussions/937\n",
    "https://github.com/guillaumekln/faster-whisper\n",
    "\n",
    "# [exploring OpenAI](https://deepgram.com/learn/exploring-whisper) \n",
    "\n",
    "https://github.com/huggingface/transformers/issues/22612\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\work\\\\Github\\\\whisper'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r'C:\\\\work\\\\Github\\\\whisper')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow  # required in Colab to avoid protobuf compatibility issues\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is multilingual and has 37,184,640 parameters.\n"
     ]
    }
   ],
   "source": [
    "model_tiny = whisper.load_model(\"tiny\")\n",
    "print(\n",
    "    f\"Model is {'multilingual' if model_tiny.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model_tiny.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 262640]) 16000\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "def load_wav_to_tensor(file_path):\n",
    "    # Load the WAV file using librosa\n",
    "    waveform, sample_rate = librosa.load(file_path, sr=None, mono=True)\n",
    "\n",
    "    # Convert the waveform to a torch tensor\n",
    "    tensor_waveform = torch.tensor(waveform).unsqueeze(0)\n",
    "\n",
    "    return tensor_waveform, sample_rate\n",
    "\n",
    "# Example usage\n",
    "file_path = r\"C:\\work\\local4test\\sampleAudio\\shortTestRecording.wav\"\n",
    "audio, sample_rate = load_wav_to_tensor(file_path)\n",
    "\n",
    "print(type(audio), audio.shape, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict without timestamps for short-form transcription\n",
    "options = whisper.DecodingOptions(language=\"en\", \n",
    "                                  without_timestamps=False, \n",
    "                                  fp16 = False)\n",
    "#                                  beam_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 262640]) 16.415\n",
      "<class 'torch.Tensor'> torch.Size([480000])\n",
      "<class 'torch.Tensor'> torch.Size([80, 3000])\n",
      "TVdbg: calling DecodingTask(model, options).run(mel)\n",
      "tvdbg True, language='en', transcribe\n",
      "tvdbg next_tokens=tensor([50364])\n",
      "tvdbg values  tensor([-0.0778, -4.7494, -5.0728, -5.2159, -5.4058, -5.5530, -5.5588, -5.5881,\n",
      "        -5.5893, -5.7145])\n",
      "tvdbg indices tensor([50364, 50376, 50372, 50374, 50375, 50378, 50373, 50368, 50380, 50370])\n",
      "tvdbg <class 'list'> [50364, 50376, 50372, 50374, 50375, 50378, 50373, 50368, 50380, 50370]\n",
      "['']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0778]) self.eot=50257\n",
      "tvdbg 0 tensor([50258, 50259, 50359, 50364])\n",
      "tvdbg next_tokens=tensor([45517])\n",
      "tvdbg values  tensor([-0.2734, -1.9554, -4.3461, -4.9601, -4.9666, -5.4980, -5.8983, -6.3826,\n",
      "        -6.6220, -6.6550])\n",
      "tvdbg indices tensor([45517,  4997,  9279,  3165,   314,  1500,   502,  6921,  1449, 11019])\n",
      "tvdbg <class 'list'> [45517, 4997, 9279, 3165, 314, 1500, 502, 6921, 1449, 11019]\n",
      "[' Testing testing Test 16 T test 1 tests Just Cast']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.2734]) self.eot=50257\n",
      "tvdbg 1 tensor([50258, 50259, 50359, 50364, 45517])\n",
      "tvdbg next_tokens=tensor([3165])\n",
      "tvdbg values  tensor([-0.0925, -3.7087, -4.2874, -4.5658, -4.5820, -6.0079, -6.6395, -6.6595,\n",
      "        -6.7519, -6.7652])\n",
      "tvdbg indices tensor([ 3165,    11, 36885,  6866, 27847,  6549, 47374,   502,    13,  4060])\n",
      "tvdbg <class 'list'> [3165, 11, 36885, 6866, 27847, 6549, 47374, 502, 13, 4060]\n",
      "[' 16, 160016 sixteen 2016 Sixt 1. 60']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0925]) self.eot=50257\n",
      "tvdbg 2 tensor([50258, 50259, 50359, 50364, 45517,  3165])\n",
      "tvdbg next_tokens=tensor([74])\n",
      "tvdbg values  tensor([-1.7834, -2.2717, -2.3560, -2.4500, -2.4639, -2.4850, -2.5085, -3.2281,\n",
      "        -3.2809, -3.6575])\n",
      "tvdbg indices tensor([   74, 21112,    42,   350,   591,    13,    11,    12,  5128, 13364])\n",
      "tvdbg <class 'list'> [74, 21112, 42, 350, 591, 13, 11, 12, 5128, 13364]\n",
      "['k kiloK k K.,- kil killer']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-1.7834]) self.eot=50257\n",
      "tvdbg 3 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74])\n",
      "tvdbg next_tokens=tensor([21409])\n",
      "tvdbg values  tensor([-0.0642, -4.7663, -5.3502, -5.7581, -5.8274, -5.8456, -6.4899, -6.5465,\n",
      "        -6.5888, -6.6314])\n",
      "tvdbg indices tensor([21409,    54,    21,    23,    49,    87,  8725,   420,    81, 39747])\n",
      "tvdbg <class 'list'> [21409, 54, 21, 23, 49, 87, 8725, 420, 81, 39747]\n",
      "['HzW68Rx Word orr Hz']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0642]) self.eot=50257\n",
      "tvdbg 4 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409])\n",
      "tvdbg next_tokens=tensor([3165])\n",
      "tvdbg values  tensor([-0.3830, -1.4646, -3.6301, -5.3444, -5.7224, -6.0661, -6.1024, -6.1538,\n",
      "        -6.1649, -6.2063])\n",
      "tvdbg indices tensor([ 3165,    11,  6866,    12,    13,  6549, 36885,   293,   485,  4060])\n",
      "tvdbg <class 'list'> [3165, 11, 6866, 12, 13, 6549, 36885, 293, 485, 4060]\n",
      "[' 16,16-. 2016 1600 and... 60']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.3830]) self.eot=50257\n",
      "tvdbg 5 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165])\n",
      "tvdbg next_tokens=tensor([65])\n",
      "tvdbg values  tensor([-1.8078, -2.1344, -2.3777, -2.4220, -2.9466, -3.1658, -3.3675, -3.5751,\n",
      "        -4.0774, -4.1012])\n",
      "tvdbg indices tensor([   65,    12,    33,  9239,   272, 16447,    85,    74,   363, 34010])\n",
      "tvdbg <class 'list'> [65, 12, 33, 9239, 272, 16447, 85, 74, 363, 34010]\n",
      "['b-B bits b beatsvk Bbits']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-1.8078]) self.eot=50257\n",
      "tvdbg 6 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65])\n",
      "tvdbg next_tokens=tensor([1373])\n",
      "tvdbg values  tensor([-1.3345, -3.2220, -3.2539, -3.2707, -3.3421, -3.5240, -3.5573, -3.5980,\n",
      "        -3.7010, -3.7931])\n",
      "tvdbg indices tensor([ 1373, 21409,  1080,  3742,  1385,    66,    89,  1208,  1024,    65])\n",
      "tvdbg <class 'list'> [1373, 21409, 1080, 3742, 1385, 66, 89, 1208, 1024, 65]\n",
      "['tsHz itsidsetsczitsatesb']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-1.3345]) self.eot=50257\n",
      "tvdbg 7 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373])\n",
      "tvdbg next_tokens=tensor([4084])\n",
      "tvdbg values  tensor([-0.4522, -1.2975, -4.6022, -4.6454, -4.6707, -5.2575, -5.6783, -5.7300,\n",
      "        -6.1741, -6.2161])\n",
      "tvdbg indices tensor([ 4084,    11,    13,  1884,  2942, 40002,   293,  7829, 14181,  8016])\n",
      "tvdbg <class 'list'> [4084, 11, 13, 1884, 2942, 40002, 293, 7829, 14181, 8016]\n",
      "[' creating,. create created Creating and creates creator creation']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.4522]) self.eot=50257\n",
      "tvdbg 8 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084])\n",
      "tvdbg next_tokens=tensor([257])\n",
      "tvdbg values  tensor([-0.0785, -4.5251, -4.9776, -5.1823, -5.3989, -5.9945, -6.1127, -6.2033,\n",
      "        -6.3469, -6.5530])\n",
      "tvdbg indices tensor([ 257,  364,  264, 6465, 2388, 9970, 5372,  316, 1882,  280])\n",
      "tvdbg <class 'list'> [257, 364, 264, 6465, 2388, 9970, 5372, 316, 1882, 280]\n",
      "[' a an the PC ep Ep AP A ap p']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0785]) self.eot=50257\n",
      "tvdbg 9 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257])\n",
      "tvdbg next_tokens=tensor([6465])\n",
      "tvdbg values  tensor([-0.3818, -1.4740, -3.2462, -4.1906, -5.4975, -6.5787, -6.8314, -7.0234,\n",
      "        -7.1264, -7.3098])\n",
      "tvdbg indices tensor([ 6465,   280,   430, 43451, 42065,  2522,   363, 46913,  4336, 32180])\n",
      "tvdbg <class 'list'> [6465, 280, 430, 43451, 42065, 2522, 363, 46913, 4336, 32180]\n",
      "[' PC p P pc PCB piece B PCs peace lithium']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.3818]) self.eot=50257\n",
      "tvdbg 10 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465])\n",
      "tvdbg next_tokens=tensor([44])\n",
      "tvdbg values  tensor([-0.0559, -4.0014, -4.2060, -4.2425, -7.5247, -7.5940, -7.6257, -7.6964,\n",
      "        -8.1390, -8.1636])\n",
      "tvdbg indices tensor([   44,    45,    76,    43,    37,   376,    77,  6683,  2865, 26386])\n",
      "tvdbg <class 'list'> [44, 45, 76, 43, 37, 376, 77, 6683, 2865, 26386]\n",
      "['MNmLF MnEMAMMs']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0559]) self.eot=50257\n",
      "tvdbg 11 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44])\n",
      "tvdbg next_tokens=tensor([281])\n",
      "tvdbg values  tensor([-0.1252, -5.0770, -5.2173, -5.2943, -5.4114, -5.4633, -5.5379, -5.6390,\n",
      "        -5.8536, -5.8631])\n",
      "tvdbg indices tensor([  281, 50664, 50670,    11, 50662, 50667, 50668, 50666, 50671, 50672])\n",
      "tvdbg <class 'list'> [281, 50664, 50670, 11, 50662, 50667, 50668, 50666, 50671, 50672]\n",
      "[' to,']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.1252]) self.eot=50257\n",
      "tvdbg 12 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281])\n",
      "tvdbg next_tokens=tensor([536])\n",
      "tvdbg values  tensor([-0.0110, -6.5887, -7.3324, -8.0321, -8.4504, -8.4515, -8.4636, -8.4659,\n",
      "        -8.4823, -8.4873])\n",
      "tvdbg indices tensor([  536,   584,   855,  8075, 50682,   383,   574,   437,   264,  1159])\n",
      "tvdbg <class 'list'> [536, 584, 855, 8075, 50682, 383, 574, 437, 264, 1159]\n",
      "[' see say show seek C look what the watch']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0110]) self.eot=50257\n",
      "tvdbg 13 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536])\n",
      "tvdbg next_tokens=tensor([437])\n",
      "tvdbg values  tensor([-0.0233, -6.4330, -6.6716, -6.8750, -6.9523, -7.0468, -7.2633, -7.3191,\n",
      "        -7.4397, -7.5999])\n",
      "tvdbg indices tensor([  437,   708,   264,   597,    11,  1281, 50714, 50706, 50710, 18090])\n",
      "tvdbg <class 'list'> [437, 708, 264, 597, 11, 1281, 50714, 50706, 50710, 18090]\n",
      "[' what What the which, water WHAT']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0233]) self.eot=50257\n",
      "tvdbg 14 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437])\n",
      "tvdbg next_tokens=tensor([264])\n",
      "tvdbg values  tensor([-0.0892, -3.6243, -4.4455, -5.0340, -5.3966, -6.9169, -7.0264, -7.3276,\n",
      "        -7.3663, -7.4267])\n",
      "tvdbg indices tensor([  264, 11150,   257,   641,   440,   428,  1163,  6101,   341,  3513])\n",
      "tvdbg <class 'list'> [264, 11150, 257, 641, 440, 428, 1163, 6101, 341, 3513]\n",
      "[' the recognition a their The your der communication this direction']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0892]) self.eot=50257\n",
      "tvdbg 15 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264])\n",
      "tvdbg next_tokens=tensor([11150])\n",
      "tvdbg values  tensor([-0.4846, -2.5971, -3.2360, -3.3161, -4.4725, -4.5653, -4.6866, -4.8902,\n",
      "        -4.9603, -5.2566])\n",
      "tvdbg indices tensor([11150, 11879,  6562,  1300,  4475, 44682, 38247,   850,  6101,  8894])\n",
      "tvdbg <class 'list'> [11150, 11879, 6562, 1300, 4475, 44682, 38247, 850, 6101, 8894]\n",
      "[' recognition recommendation combination Re organization Recogn technician rec communication revolution']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.4846]) self.eot=50257\n",
      "tvdbg 16 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150])\n",
      "tvdbg next_tokens=tensor([2709])\n",
      "tvdbg values  tensor([-0.0267, -5.1849, -6.4747, -6.5132, -6.5263, -6.7639, -7.4383, -7.5503,\n",
      "        -7.7098, -8.0280])\n",
      "tvdbg indices tensor([ 2709,   976,  2729,   307,   290,  3417, 11215,  1709,   775,   390])\n",
      "tvdbg <class 'list'> [2709, 976, 2729, 307, 290, 3417, 11215, 1709, 775, 390]\n",
      "[' gives give gave is g feels deals goes does was']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0267]) self.eot=50257\n",
      "tvdbg 17 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709])\n",
      "tvdbg next_tokens=tensor([505])\n",
      "tvdbg values  tensor([-0.0088, -5.6095, -7.4256, -8.0701, -8.3642, -8.4616, -8.5368, -8.6144,\n",
      "        -8.6469, -8.7933])\n",
      "tvdbg indices tensor([  505,   382,   281,   257,    13,   385,    11, 50792, 50796, 50798])\n",
      "tvdbg <class 'list'> [505, 382, 281, 257, 13, 385, 11, 50792, 50796, 50798]\n",
      "[' us as to a. me,']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0088]) self.eot=50257\n",
      "tvdbg 18 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505])\n",
      "tvdbg next_tokens=tensor([257])\n",
      "tvdbg values  tensor([-0.8451, -1.4019, -1.4652, -4.9437, -5.0521, -5.2884, -5.3946, -5.8855,\n",
      "        -6.1584, -6.2138])\n",
      "tvdbg indices tensor([  257,    13,    11,   382,     0,  1662,   485, 50814, 50819,   316])\n",
      "tvdbg <class 'list'> [257, 13, 11, 382, 0, 1662, 485, 50814, 50819, 316]\n",
      "[' a., as! –... A']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.8451]) self.eot=50257\n",
      "tvdbg 19 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257])\n",
      "tvdbg next_tokens=tensor([2099])\n",
      "tvdbg values  tensor([-0.0097, -6.6376, -6.7679, -7.3529, -7.5876, -7.6527, -7.6773, -8.6974,\n",
      "        -8.7228, -8.7488])\n",
      "tvdbg indices tensor([ 2099, 16881, 24822,  3347,   402,  8199, 11639,  1160, 31875,  5507])\n",
      "tvdbg <class 'list'> [2099, 16881, 24822, 3347, 402, 8199, 11639, 1160, 31875, 5507]\n",
      "[' short Short shortcut shot sh sharp shorter Sh shortest shared']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0097]) self.eot=50257\n",
      "tvdbg 20 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257,  2099])\n",
      "tvdbg next_tokens=tensor([3636])\n",
      "tvdbg values  tensor([-0.0443, -4.9472, -5.8123, -6.0160, -6.2483, -6.2619, -6.4302, -6.5619,\n",
      "        -6.7143, -6.8267])\n",
      "tvdbg indices tensor([ 3636, 45947,    11, 16145, 50864,    12,  7897, 50862, 50858, 50860])\n",
      "tvdbg <class 'list'> [3636, 45947, 11, 16145, 50864, 12, 7897, 50862, 50858, 50860]\n",
      "[' message Message, massage- messages']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0443]) self.eot=50257\n",
      "tvdbg 21 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257,  2099,  3636])\n",
      "tvdbg next_tokens=tensor([337])\n",
      "tvdbg values  tensor([-0.0548, -5.2612, -5.7292, -5.9041, -6.0304, -6.0606, -6.1039, -6.1922,\n",
      "        -6.3753, -6.7864])\n",
      "tvdbg indices tensor([  337,    13, 50892,    11, 50890, 50889,   281,   295,   490, 50887])\n",
      "tvdbg <class 'list'> [337, 13, 50892, 11, 50890, 50889, 281, 295, 490, 50887]\n",
      "[' for., to of from']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0548]) self.eot=50257\n",
      "tvdbg 22 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257,  2099,  3636,   337])\n",
      "tvdbg next_tokens=tensor([4997])\n",
      "tvdbg values  tensor([-0.0443, -4.5247, -5.8028, -6.4174, -7.0306, -7.0417, -7.0568, -7.1535,\n",
      "        -7.1987, -7.1999])\n",
      "tvdbg indices tensor([ 4997, 45517,  1500,   264, 50904,  6921, 50906, 50910, 26223, 50900])\n",
      "tvdbg <class 'list'> [4997, 45517, 1500, 264, 50904, 6921, 50906, 50910, 26223, 50900]\n",
      "[' testing Testing test the tests tasting']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0443]) self.eot=50257\n",
      "tvdbg 23 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257,  2099,  3636,   337,  4997])\n",
      "tvdbg next_tokens=tensor([13])\n",
      "tvdbg values  tensor([-0.2929, -4.5561, -4.9825, -5.0055, -5.1104, -5.2192, -5.3729, -5.4072,\n",
      "        -5.4328, -5.4391])\n",
      "tvdbg indices tensor([   13,     0, 50934, 50936, 50932, 50940, 50937, 50928, 50942,    11])\n",
      "tvdbg <class 'list'> [13, 0, 50934, 50936, 50932, 50940, 50937, 50928, 50942, 11]\n",
      "['.!,']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.2929]) self.eot=50257\n",
      "tvdbg 24 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257,  2099,  3636,   337,  4997,    13])\n",
      "tvdbg next_tokens=tensor([50964])\n",
      "tvdbg values  tensor([-3.8223, -4.0706, -4.1772, -4.2583, -4.2893, -4.3009, -4.3106, -4.3368,\n",
      "        -4.3382, -4.3403])\n",
      "tvdbg indices tensor([50964, 50940, 50942, 50954, 50951, 50948, 50944, 50956, 50939, 50949])\n",
      "tvdbg <class 'list'> [50964, 50940, 50942, 50954, 50951, 50948, 50944, 50956, 50939, 50949]\n",
      "['']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-3.8223]) self.eot=50257\n",
      "tvdbg 25 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257,  2099,  3636,   337,  4997,    13, 50964])\n",
      "tvdbg next_tokens=tensor([50257])\n",
      "tvdbg values  tensor([-0.0172, -5.2604, -6.5525, -6.6675, -6.8638, -7.0623, -7.8012, -8.7459,\n",
      "        -8.9538, -9.0539])\n",
      "tvdbg indices tensor([50257, 50964, 51164, 51064, 51114, 51014, 51214, 50989, 50965, 50966])\n",
      "tvdbg <class 'list'> [50257, 50964, 51164, 51064, 51114, 51014, 51214, 50989, 50965, 50966]\n",
      "['<|endoftext|>']\n",
      "tvdbg <class 'torch.Tensor'> torch.Size([1]) tensor([-0.0172]) self.eot=50257\n",
      "tvdbg 26 tensor([50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,\n",
      "         1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150,\n",
      "         2709,   505,   257,  2099,  3636,   337,  4997,    13, 50964, 50257])\n",
      "TVdbg: <class 'int'> 0\n",
      "-12.684431076049805\n",
      "['Testing 16kHz 16bts creating a PCM to see what the recognition gives us a short message for testing.']\n",
      "TVdbg: <class 'list'> [0]\n",
      "|Testing 16kHz 16bts creating a PCM to see what the recognition gives us a short message for testing.|\n",
      "CPU times: total: 2.61 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "hypotheses = []\n",
    "print(type(audio), audio.shape, f\"{audio.shape[1]/16000}\")\n",
    "w_audio = whisper.pad_or_trim(audio.flatten())\n",
    "print(type(w_audio), w_audio.shape)\n",
    "mel = whisper.log_mel_spectrogram(w_audio)\n",
    "print(type(mel), mel.shape)\n",
    "results = model_tiny.decode(mel, options)\n",
    "#hypotheses.extend([result.text for result in results])\n",
    "print(f\"|{results.text}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# current code\n",
    "\n",
    "whisper/decoding.py\n",
    "\n",
    "    class GreedyDecoder(TokenDecoder):\n",
    "        def __init__(self, temperature: float, eot: int):\n",
    "            self.temperature = temperature\n",
    "            self.eot = eot\n",
    "            self.tokenizer = get_tokenizer(True, language='en', task='transcribe')\n",
    "    \n",
    "    \n",
    "        def update(\n",
    "            self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n",
    "        ) -> Tuple[Tensor, bool]:\n",
    "            if self.temperature == 0:\n",
    "                next_tokens = logits.argmax(dim=-1)\n",
    "            else:\n",
    "                next_tokens = Categorical(logits=logits / self.temperature).sample()\n",
    "    \n",
    "            print(f\"tvdbg {next_tokens=}\")\n",
    "            logprobs = F.log_softmax(logits.float(), dim=-1)\n",
    "            #print(f\"tvdbg {logprobs[0][:10]}\")\n",
    "            sorted_tensor, sorted_indices = torch.sort(logprobs[0][:], descending=True)\n",
    "            print(f\"tvdbg values  {sorted_tensor[:10]}\")\n",
    "            print(f\"tvdbg indices {sorted_indices[:10]}\")\n",
    "            #print(\"tvdbg {[self.tokenizer.decode([[t.item()]]).strip() for t in sorted_indices[:10]]}\")\n",
    "            #print(f\"tvdbg {[self.tokenizer.decode([[t.item()]]).strip() for t in sorted_indices[:10]]}\")\n",
    "            #print(f\"tvdbg {[t for t in sorted_indices[:10]]}\")\n",
    "            #tokens1: List[List[int]] = [t[i].tolist() for i, t in zip([selected], tokens)]\n",
    "            #tokens1 = [50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,   1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150, 2709,   505,   257,  2099,  3636,   337,  4997,    13, 50964, 50257]\n",
    "            tokens1: List[List[int]] = [t.item() for t in sorted_indices[:10]]\n",
    "            print(f\"tvdbg {type(tokens1)} {tokens1}\")\n",
    "            #texts: List[str] = [self.tokenizer.decode(t).strip() for t in [tokens1]]\n",
    "            texts: List[str] = [self.tokenizer.decode(t) for t in [tokens1]]\n",
    "            print(texts)\n",
    "    \n",
    "    \n",
    "            current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n",
    "            print(f\"tvdbg {type(current_logprobs)} {current_logprobs.shape} {current_logprobs[:10]} {self.eot=}\")\n",
    "            sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n",
    "    \n",
    "            next_tokens[tokens[:, -1] == self.eot] = self.eot\n",
    "            tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n",
    "    \n",
    "            completed = (tokens[:, -1] == self.eot).all()\n",
    "            return tokens, completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remember \n",
    "\n",
    "whisper/decoding.py\n",
    "\n",
    "tokens1 = [50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,   1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150, 2709,   505,   257,  2099,  3636,   337,  4997,    13, 50964, 50257]\n",
    "        print(f\"tvdbg {type(tokens1)} \")\n",
    "        texts: List[str] = [self.tokenizer.decode(t).strip() for t in [tokens1]]\n",
    "        print(texts)\n",
    "        \n",
    "['<|startoftranscript|><|en|><|transcribe|> Testing 16kHz 16bts creating a PCM to see what the recognition gives us a short message for testing.<|endoftext|>']\n",
    "\n",
    "\n",
    "    class GreedyDecoder(TokenDecoder):\n",
    "        def __init__(self, temperature: float, eot: int):\n",
    "            self.temperature = temperature\n",
    "            self.eot = eot\n",
    "            self.tokenizer = get_tokenizer(True, language='en', task='transcribe')\n",
    "    \n",
    "    \n",
    "        def update(\n",
    "            self, tokens: Tensor, logits: Tensor, sum_logprobs: Tensor\n",
    "        ) -> Tuple[Tensor, bool]:\n",
    "            if self.temperature == 0:\n",
    "                next_tokens = logits.argmax(dim=-1)\n",
    "            else:\n",
    "                next_tokens = Categorical(logits=logits / self.temperature).sample()\n",
    "    \n",
    "            print(f\"tvdbg {next_tokens=}\")\n",
    "            logprobs = F.log_softmax(logits.float(), dim=-1)\n",
    "            #print(f\"tvdbg {logprobs[0][:10]}\")\n",
    "            sorted_tensor, sorted_indices = torch.sort(logprobs[0][:], descending=True)\n",
    "            print(f\"tvdbg values  {sorted_tensor[:10]}\")\n",
    "            print(f\"tvdbg indices {sorted_indices[:10]}\")\n",
    "            #print(\"tvdbg {[self.tokenizer.decode([[t.item()]]).strip() for t in sorted_indices[:10]]}\")\n",
    "            #print(f\"tvdbg {[self.tokenizer.decode([[t.item()]]).strip() for t in sorted_indices[:10]]}\")\n",
    "            #print(f\"tvdbg {[t for t in sorted_indices[:10]]}\")\n",
    "            #tokens1: List[List[int]] = [t[i].tolist() for i, t in zip([selected], tokens)]\n",
    "            tokens1 = [50258, 50259, 50359, 50364, 45517,  3165,    74, 21409,  3165,    65,   1373,  4084,   257,  6465,    44,   281,   536,   437,   264, 11150, 2709,   505,   257,  2099,  3636,   337,  4997,    13, 50964, 50257]\n",
    "            print(f\"tvdbg {type(tokens1)} \")\n",
    "            texts: List[str] = [self.tokenizer.decode(t).strip() for t in [tokens1]]\n",
    "            print(texts)\n",
    "    \n",
    "    \n",
    "            current_logprobs = logprobs[torch.arange(logprobs.shape[0]), next_tokens]\n",
    "            print(f\"tvdbg {type(current_logprobs)} {current_logprobs.shape} {current_logprobs[:10]} {self.eot=}\")\n",
    "            sum_logprobs += current_logprobs * (tokens[:, -1] != self.eot)\n",
    "    \n",
    "            next_tokens[tokens[:, -1] == self.eot] = self.eot\n",
    "            tokens = torch.cat([tokens, next_tokens[:, None]], dim=-1)\n",
    "    \n",
    "            completed = (tokens[:, -1] == self.eot).all()\n",
    "            return tokens, completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [silero-vad](https://github.com/snakers4/silero-vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/snakers4/silero-vad/blob/master/silero-vad.ipynb#scrollTo=pSifus5IilRp\n",
    "\n",
    "#https://github.com/snakers4/silero-vad/blob/master/utils_vad.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "from pprint import pprint\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_ONNX = False # change this to True if you want to test onnx model\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',\n",
    "                              model='silero_vad',\n",
    "                              force_reload=True,\n",
    "                              onnx=USE_ONNX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = read_audio(file_path, sampling_rate=SAMPLING_RATE)\n",
    "# get speech timestamps from full audio file\n",
    "speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=SAMPLING_RATE, visualize_probs=False)\n",
    "pprint(speech_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg in speech_timestamps:\n",
    "    print(f\"{seg['start']/SAMPLING_RATE} to {seg['end']/SAMPLING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "#plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(wav.numpy(), sr=SAMPLING_RATE)\n",
    "#plt.xlabel(\"Time (s)\")\n",
    "#plt.ylabel(\"Amplitude\")\n",
    "#plt.title(\"Audio Waveform\")\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(librosa.ex('choice'), duration=10)\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
    "librosa.display.waveshow(wav.numpy(), sr=SAMPLING_RATE, ax=ax[0])\n",
    "ax[0].set(title='Envelope view, mono')\n",
    "ax[0].label_outer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Create a tensor of logits (unnormalized probabilities)\n",
    "logits = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# Create a Categorical distribution\n",
    "distribution = Categorical(logits=logits)\n",
    "\n",
    "# Sample from the distribution\n",
    "sample = distribution.sample()  # Returns an index (0, 1, or 2)\n",
    "\n",
    "# Compute the log probability of a specific value (e.g., index 2)\n",
    "log_prob = distribution.log_prob(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "\n",
    "tokenizer = whisper.tokenizer()\n",
    "\n",
    "t = tokenizer.encode(\"This is a string\")\n",
    "\n",
    "decoded_string = tokenizer.decode(t).strip()\n",
    "\n",
    "print(decoded_string)\n",
    "# This is a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "039b53f2702c4179af7e0548018d0588": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06b9aa5f49fa44ba8c93b647dc7db224": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0d10a42c753453283e5219c22239337",
      "max": 164,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_09f4cb79ff86465aaf48b0de24869af9",
      "value": 164
     }
    },
    "09a29a91f58d4462942505a3cc415801": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83391f98a240490987c397048fc1a0d4",
       "IPY_MODEL_06b9aa5f49fa44ba8c93b647dc7db224",
       "IPY_MODEL_da9c231ee67047fb89073c95326b72a5"
      ],
      "layout": "IPY_MODEL_48da931ebe7f4fd299f8c98c7d2460ff"
     }
    },
    "09f4cb79ff86465aaf48b0de24869af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b9cecf5b3584fba8258a81d4279a25b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39f5a6ae8ba74c8598f9c6d5b8ad2d65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48da931ebe7f4fd299f8c98c7d2460ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a901f447c1d477bb49f954e0feacedd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83391f98a240490987c397048fc1a0d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a901f447c1d477bb49f954e0feacedd",
      "placeholder": "​",
      "style": "IPY_MODEL_39f5a6ae8ba74c8598f9c6d5b8ad2d65",
      "value": "100%"
     }
    },
    "a0d10a42c753453283e5219c22239337": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da9c231ee67047fb89073c95326b72a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9cecf5b3584fba8258a81d4279a25b",
      "placeholder": "​",
      "style": "IPY_MODEL_039b53f2702c4179af7e0548018d0588",
      "value": " 164/164 [05:08&lt;00:00,  1.86s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
